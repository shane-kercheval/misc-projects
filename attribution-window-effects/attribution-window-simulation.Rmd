---
title: "README"
author: "Shane Kercheval"
output:
    md_document:
    variant: markdown_github
toc: true
toc_depth: 4
---
    
```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(knitr)
library(tidyverse)
library(lubridate)
library(scales)
options(scipen=999) # non-scientific notation

knitr::opts_chunk$set(echo = FALSE)
```

# What are the effects of diminishing lift from an A/B test and the use of attribution windows?

A few definitions:

- `control`: refers to either the `A` group, or the thing you show to the `A` group
- `variant`: refers to either the `B` group, or the thing you show to the `B` group
- `variation`: refers to either the `A` (`control`) group or `B` (`variant`).
- `lift`: the percent increase from the `A` group to the `B` based on the thing you are testing in the `B` group (i.e. the `variant`)
- `attribution window` the maximum number of days you allow conversions to be attributed to the corresponding variation based on when the person entered the experiment. So person `x` enters the experiment on day `y`. With a `7-day` attribution window, we would only count `x`'s conversion if the conversion event took place within `7` days of `y`. This would be the same for both variations.

## There are two questions when running A/B tests that don't seem to be discussed much.

The first question is: *Should I use an attribution window, and if so, how long?*

In other words, how much time should I allow for a conversion to be attributed to the corresponding variation of the experiment?

The typical process for A/B testing doesn't seem to consider/use an attribution window: You run a test for, say, 30 days (depending on your sample size calculator). The people who enter the experiment on the first day it runs get 30 days to convert (i.e. their conversion event is counted at any point during the experiment). The people who enter into the experiment on the last day get <= 1 day to convert. Perhaps some teams give an extra, say, 7 days for conversion (so people who see the experiment on the first day, get 37 days, people who see it on the last day get 7.).

One assumption is, the fact that some people get more time and some people get less time to convert doesn't matter, because of the randomization of the control and the variant.

A related question is: *How does the diminishing lift, over time, from the variant, affect the outcome of the experiment and what effect does using an attribution window have?*

My assumption is that there is a diminishing effect of the variant on the person's decision-making process or actions, over time, relative to one the person first saw the variant. In other words, for *most* A/B experiments, most people will see the variant (e.g. certain button, page, or experience that is different from what people in the control saw) and the impact (i.e. the *lift*; the impact on the person compared to what they would have experienced had they been in the control group) that the particular variant gives is highest in the first moments that it is experienced, but that impact (exponentially?) decreases over time. At the extreme, if a person "converts" (e.g. buys your product/service) a year after seeing the variant, that variant had virtually nothing to do with conversion. Depnding on the experiment/variant, we can probably make similar statements for (perhaps) days, (mostly likely) weeks, and (certainly) months after the variant is seen.***

The concern is that the longer you give people to convert, the less *lift* there is, and you're essentially including people who are, at that point, converting at the same rate as the control group. This introduces noise into your experiment. Can we use an attributionw window to reduce the noise? Will the attribution window also reduce the signal?

***Perhaps this means that A/B testing is inherently targeting people that either A) more suspectible to pschogolical manipulation (e.g. mind hacks, high pressure sales tactics, buying things based on whether it's sunny or dark outside, etc.) and/or B) wanting to make an immediately decision C) impulsive buyers. But, that's a direction I don't want to go right now. The caveat to this is that, there are of course experiments that you run where, for example, you've completely redesigned your product and that new experience does have a long lasting impact on the customers decision. In those cases, the lift is high enough that any concerns we deal with here are probably going to disappear from the large lift.

## Simulate A/B Groups

```{r}
num_trials <- 40000
seed_traffic <- 1
seed_conversion <- 4
```

- We'll create ``r comma_format()(num_trials)`` trials in experiment, roughly ``r comma_format()(num_trials / 2)`` per randomized variation.
- Then we'll randomly assign people into an `A` group or `B` group based on a random number generator (0/1).

```{r create_traffic}
set.seed(seed_traffic)
traffic <- data.frame(id=sample(1:1000000, num_trials, replace=F))

set.seed(seed_traffic)
traffic$variation <- ifelse(rbinom(num_trials, 1, 0.5), 'A', 'B')

traffic %>% 
    count(variation) %>%
    ggplot(aes(x=variation, y=n, fill=variation)) +
    geom_col() +
    geom_text(aes(label=comma_format()(n)), vjust = -0.4) +
    theme_light() +
    theme(legend.position = 'none') +
    scale_y_continuous(labels = comma_format()) +
    labs(title='Number of people (trials) in each variation',
         y='Trials',
         x='Variation')
```

```{r create_conversion_rate}
baseline_conversion_rate <- 0.20
traffic$baseline_conversion_rate <- baseline_conversion_rate
```

## Simulate Conversion Rates

Now, we'll give everyone a baseline conversion rate of ``r percent(baseline_conversion_rate)``.

Specifically, we'll assign each person a conversion rate (i.e. probability) then take a random sample from
the binomial distribution using othat probabilty to determine if that person converted.

Let's simulate the conversion for each person and see the overall conversion rates for each variation.

```{r create_conversion_rate2}
converted_by_id <- function(id, conversion_rate) {
    #set.seed(id)
    return (as.logical(rbinom(1, 1, conversion_rate)))
}
set.seed(seed_conversion)
traffic$converted <- map2_lgl(traffic$id, traffic$baseline_conversion_rate, ~ converted_by_id(.x, .y))

conversion_table <- table(traffic$variation, traffic$converted)
conversions <- c(conversion_table['A', 'TRUE'], conversion_table['B', 'TRUE'])
non_conversions <- c(conversion_table['A', 'FALSE'], conversion_table['B', 'FALSE'])
trials <- conversions + non_conversions

traffic %>% 
    count(variation, converted) %>%
    group_by(variation) %>%
    mutate(baseline_conversion_rate = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=variation, y=n, fill=converted)) +
    scale_y_continuous(labels = comma_format()) +
    geom_col(position='dodge') +
    geom_text(aes(label=comma_format()(n)),
              position = position_dodge(width=0.9),
              vjust = 1.2) +
    geom_text(aes(label=percent_format()(baseline_conversion_rate)),
              position = position_dodge(width=0.9),
              vjust = -0.4) +
    labs(title='Simulated 20% (Baseline) Conversion Rate from Binomial Distribution',
         y='Trials',
         x='Variation',
         fill='Converted') +
    theme_light()
```

```{r }
simulated_percent_increase_in_b <- 0.07
```

P-value of above conversions: ``r prop.test(x=conversions, n=trials)$p.value``

What we've done so far has simulated an experiment in which there was **no** change in the `A` group vs the `B` group. It was an experiment that might as well been an `A`/`A` test.

Now, let's simulate everyone in the `B` having a ``r percent(simulated_percent_increase_in_b)`` increase (due from the affects of the new thing we are testing out.)

```{r}
traffic <- traffic %>%
    mutate(lift = ifelse(variation == 'A', 0, simulated_percent_increase_in_b)) %>%
    mutate(conversion_rate = baseline_conversion_rate + (baseline_conversion_rate * lift)) %>%
    select(-converted)

set.seed(seed_conversion)
converted_no_diminishing_effects <- map2_lgl(traffic$id, traffic$conversion_rate, ~ converted_by_id(.x, .y))
traffic$converted <- converted_no_diminishing_effects

conversion_table <- table(traffic$variation, traffic$converted)
conversions <- c(conversion_table['A', 'TRUE'], conversion_table['B', 'TRUE'])
non_conversions <- c(conversion_table['A', 'FALSE'], conversion_table['B', 'FALSE'])
trials <- conversions + non_conversions

a_conversion_rate <- conversions[1] / trials[1]
b_conversion_rate <- conversions[2] / trials[2]

b_percent_increase_over_baseline <- (b_conversion_rate - baseline_conversion_rate) / b_conversion_rate
b_percent_increase_over_a <- (b_conversion_rate - a_conversion_rate) / b_conversion_rate

traffic %>% 
    count(variation, converted) %>%
    group_by(variation) %>%
    mutate(conversion_rate = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=variation, y=n, fill=converted)) +
    geom_col(position='dodge') +
    scale_y_continuous(labels = comma_format()) +
    geom_text(aes(label=comma_format()(n)),
              position = position_dodge(width=0.9),
              vjust = 1.2) +
    geom_text(aes(label=percent_format()(conversion_rate)),
              position = position_dodge(width=0.9),
              vjust = -0.4) +
    labs(title=paste('Simulated', percent(baseline_conversion_rate),'Baseline Conversion Rate from Binomial Distribution,\nwith', percent(simulated_percent_increase_in_b),'Lift from Effects of B variation.'),
         y='Trials',
         x='Variation',
         fill='Converted') +
    theme_light()
```

Due to random varaiation (i.e. random sampling from binomial distribution), `B`'s conversion rate of ``r percent(b_conversion_rate)`` is a ``r percent(b_percent_increase_over_baseline)`` increase over the **true*** baseline conversion rate of ``r percent(baseline_conversion_rate)`` (that, because we are simulating, we know to be the true conversion rate), and a ``r percent(b_percent_increase_over_a)`` increase over `A`'s conversion rate of ``r percent(a_conversion_rate)``.

The P-value is now: ``r prop.test(x=conversions, n=trials)$p.value``

## But, here's the problem.

These conversion rates assume that everyone that has converted, have been given any amount of time to convert, **and regardless of how long after they converted (relative to when they entered the experiment), they had the same lift from the experiment**.

This doesn't take into account:

- the fact that people take different lengths of time to convert (relative to when they entered into the experiment) and 
- **the effects that that the A/B test has on a person, or group of people, diminishes over time.** It cannot be assumed that the effect of seeing a certain button, or certain design, or a certain experience, will have the same affect in that instant vs. 1 hour later vs. 1 day later vs. 30 days later. The effect of that "thing" on a person's decision or action will almost always, for almost everyone, diminish over time.

If we can assume this is a safe assumption, the question is: **What are the effects of including conversion events from people who saw the experiment "a long time ago"?**

We have to do 2 things.

First, we have to **simulate people converting at different lengths of time, relative to when they saw the experiment**. (We'll assume everyone in the experiment has not already converted; it won't affect the outcome.)

Second, we have to **simulate the effect of the experiment diminishing over time**. 

```{r}
scale_a_b <- function(x, a, b) {

    (b - a) * ((x - min(x)) / (max(x) - min(x))) + a 
}

set.seed(44)
#offset <- round(scale_a_b(rlnorm(n=num_trials, meanlog = 0.04), 0, 50))
#hist(offset)
offset <- round(scale_a_b(rgamma(num_trials, shape = 1.7), 1, 75))
offset <- ifelse(offset <= 1, 2, offset)
offset <- offset - 2



set.seed(NULL)
traffic <- traffic %>%
    mutate(conversion_offset = offset,
           #conversion_offset = ifelse(conversion_offset > 30, round(runif(1, min=20, max=30)), conversion_offset),
           conversion_offset = ifelse(conversion_offset < 0, 0, conversion_offset))

traffic$conversion_offset <- map_dbl(traffic$conversion_offset, ~ {
    if(. > 30) {
        return (round(runif(1, min=20, max=30)))
    } else {
        return (.)
    }
})

traffic %>%
    group_by(conversion_offset, variation) %>%
    summarise(num_trials=n(),
              average_conversion_rate = mean(conversion_rate)) %>%
    ggplot(aes(x=conversion_offset, y=num_trials, fill=average_conversion_rate)) +
    geom_col() +
    scale_y_continuous(labels = comma_format()) +
    scale_x_continuous(breaks = seq(0, 100)) +
    scale_fill_gradient(labels = percent) +
    facet_wrap(~variation, nrow=2) +
    labs(title='Simulated Distribution of Days from Entering Experiment to Conversion.',
         subtitle = 'This shows the number of people randomly assigned to various days;\nthey will convert at the conversion rate shown in the legend. ',
         y='Number of people/trials',
         x='Number of Days from Entering Experiment to Conversion',
         fill='Conversion Rate')
```

Now, let's simulate a diminishing effect fromt the A/B test over time.

```{r}
stand_dev <- .5
scale_norm <- 1 / dnorm(0, sd=stand_dev)

relative_effect <- function(day_number) {
    return (dnorm(((day_number) / 30 * 2), sd=stand_dev) * scale_norm)
}


data.frame(days_from_experiment_to_conversion=0:30) %>%
    mutate(relative_effect = relative_effect(days_from_experiment_to_conversion)) %>%
    ggplot(aes(x=days_from_experiment_to_conversion, y=relative_effect)) +
    geom_line() +
    geom_point() +
    geom_text(aes(label=percent(relative_effect)), vjust=-0.5, check_overlap = TRUE) +
    scale_x_continuous(breaks = 0:30) +
    labs(title='Percent of Lift Applied to the Days From Entering Experiment to Conversion.',
         subtitle='The Lift will be multiplied by this percentage for the corresponding day of conversion.',
         y='Relative Effect (Lift * y)',
         x='Number of Days from Experiment to Conversion Event.')
```

Now let's apply the "Percent of Lift Applied" to the Baseline Conversion Rate and the Lift in the Variant.

```{r}
traffic <- traffic %>%
    mutate(diminished_conversion_rate = baseline_conversion_rate + (baseline_conversion_rate * lift * relative_effect(conversion_offset)))
# %>%
#     select(-converted)

stopifnot(all(ifelse(traffic$variation == "A", traffic$baseline_conversion_rate == traffic$conversion_rate, TRUE)))
stopifnot(all(ifelse(traffic$variation == "A", traffic$baseline_conversion_rate == traffic$diminished_conversion_rate, TRUE)))

traffic %>%
    # filter()
    # filter(!is.na(conversion_offset)) %>%
    group_by(conversion_offset, variation) %>%
    summarise(diminished_conversion_rate = mean(diminished_conversion_rate)) %>%
 #   spread(variation, diminished_conversion_rate) %>%
    ggplot(aes(x=conversion_offset, y=diminished_conversion_rate, color=variation)) +
    geom_line() +
    geom_point() +
    geom_text(aes(label=percent_format()(diminished_conversion_rate)), check_overlap=TRUE, size = 3, vjust=-0.5) + 
    scale_y_continuous(labels = percent_format()) +
    #expand_limits(y=0) +
    labs(title="Diminishing Effect of Variant",
         y='Conversion Rate',
         x='Days from Entering Experiment to Conversion',
         color='Variation')

traffic %>%
    group_by(conversion_offset, variation) %>%
    summarise(num_trials=n(),
              average_conversion_rate = mean(diminished_conversion_rate)) %>%
    ggplot(aes(x=conversion_offset, y=num_trials, fill=average_conversion_rate)) +
    geom_col() +
    scale_y_continuous(labels = comma_format()) +
    scale_x_continuous(breaks = seq(0, 100)) +
    scale_fill_gradient(labels = percent) +
    facet_wrap(~variation, nrow=2) +
    labs(title='Simulated Distribution of Days from Entering Experiment to Conversion.',
         subtitle = 'This shows the number of people randomly assigned to various days;\nthey will convert at the conversion rate shown in the legend.\nThe longer it takes a person in the B group to convert, the less Lift they receive from the variant.',
         y='Number of people/trials',
         x='Number of Days from Entering Experiment to Conversion',
         fill='Conversion Rate')
```

*This is the key point*. The longer you give people in the experiment to convert, the less effect the **lift** from experiment has, and the closer the conversion rate becomes to the `A` group.

No such effect exists in the `A` group because they maintain the baseline conversion rate (there is nothing to diminish.)

But letting people convert e.g. 20-30 days after the experiment, you will definitely capture the people who take longer to convert, but the *majority* of people who convert are no longer affected by the experiment, **so they are now converting at a similar rate as the `A` group***, which add noise into the experiment. 

Another way of saying this is it's likely that the people who converted after 30 days were going to do so anyway, with or without the thing you tested (or probably the majority of nauances and brain hacks in the website).

So let's look at the results of the A/B test with the updated conversion rates.

```{r}
set.seed(seed_conversion)
new_converted <- map2_lgl(traffic$id, traffic$diminished_conversion_rate, ~ converted_by_id(.x, .y))

# ensure that conversions of variation A don't change
stopifnot(all(ifelse(traffic$variation == "A", converted_no_diminishing_effects == new_converted, TRUE)))

traffic$converted <- new_converted
#set.seed(2)
#traffic$converted <-as.logical(rbinom(num_trials, 1, baseline_conversion_rate))

conversion_table <- table(traffic$variation, traffic$converted)
conversions <- c(conversion_table['A', 'TRUE'], conversion_table['B', 'TRUE'])
non_conversions <- c(conversion_table['A', 'FALSE'], conversion_table['B', 'FALSE'])
trials <- conversions + non_conversions

a_conversion_rate <- conversions[1] / trials[1]
b_conversion_rate <- conversions[2] / trials[2]

b_percent_increase_over_baseline <- (b_conversion_rate - baseline_conversion_rate) / b_conversion_rate
b_percent_increase_over_a <- (b_conversion_rate - a_conversion_rate) / b_conversion_rate

traffic %>% 
    count(variation, converted) %>%
    group_by(variation) %>%
    mutate(conversion_rate = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=variation, y=n, fill=converted)) +
    geom_col(position='dodge') +
    scale_y_continuous(labels = comma_format()) +
    geom_text(aes(label=comma_format()(n)),
              position = position_dodge(width=0.9),
              vjust = 1.2) +
    geom_text(aes(label=percent_format()(conversion_rate)),
              position = position_dodge(width=0.9),
              vjust = -0.4) +
    labs(title=paste('Simulated', percent(baseline_conversion_rate),'Baseline Conversion Rate from Binomial Distribution,\nwith (diminishing)', percent(simulated_percent_increase_in_b),'Lift from Effects of B variation.'),
         subtitle='This is the conversion rate after simulated the effect of diminishing Lift over time.',
         y='Trials',
         x='Variation',
         fill='Converted') +
    theme_light()
```

Including the people in the experiment that converted many days after they saw the experiment, seems to dilute the conversion rate. Again, more and more people are converting at the same conversion rate as the `A` group. 

Now, `B`'s conversion rate of ``r percent(b_conversion_rate)`` is a only ``r percent(b_percent_increase_over_baseline)`` increase over the **true** baseline conversion rate of ``r percent(baseline_conversion_rate)``, and a ``r percent(b_percent_increase_over_a)`` increase over `A`'s conversion rate of ``r percent(a_conversion_rate)``, which hasn't changed since the last simulation.

The P-value is no longer statistically significant: ``r prop.test(x=conversions, n=trials)$p.value``

## But this doesn't answer the question of the affects of attribution windows. 

```{r}
attribution_window_days <- 7
```

Let's simulate a `r attribution_window_days`-day attribution window (i.e. only counting the the conversions that happen within `r attribution_window_days` days.)

```{r}
traffic <- traffic %>%
    mutate(converted_within_window = ifelse(conversion_offset <= attribution_window_days, converted, FALSE))

conversion_table <- table(traffic$variation, traffic$converted_within_window)
conversions <- c(conversion_table['A', 'TRUE'], conversion_table['B', 'TRUE'])
non_conversions <- c(conversion_table['A', 'FALSE'], conversion_table['B', 'FALSE'])
trials <- conversions + non_conversions

a_conversion_rate <- conversions[1] / trials[1]
b_conversion_rate <- conversions[2] / trials[2]

b_percent_increase_over_baseline <- (b_conversion_rate - baseline_conversion_rate) / b_conversion_rate
b_percent_increase_over_a <- (b_conversion_rate - a_conversion_rate) / b_conversion_rate

traffic %>% 
    count(variation, converted_within_window) %>%
    group_by(variation) %>%
    mutate(conversion_rate = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=variation, y=n, fill=converted_within_window)) +
    geom_col(position='dodge') +
    scale_y_continuous(labels = comma_format()) +
    geom_text(aes(label=comma_format()(n)),
              position = position_dodge(width=0.9),
              vjust = 1.2) +
    geom_text(aes(label=percent_format()(conversion_rate)),
              position = position_dodge(width=0.9),
              vjust = -0.4) +
    labs(title=paste0('Simulated ', percent(baseline_conversion_rate),' Baseline Conversion Rate from Binomial Distribution,\nwith (diminishing) ', percent(simulated_percent_increase_in_b),' Lift from Effects of B variation &\n', attribution_window_days,'-Day Attribution Window.'),
         subtitle=paste0('This is the conversion rate after simulated the effect of diminishing Lift over time, with a', attribution_window_days,'-Day Attribution Window.'),
         y='Trials',
         x='Variation',
         fill='Converted') +
    theme_light()
```

The P-value is once again statistically significant: ``r prop.test(x=conversions, n=trials)$p.value``.

In this case, it appears we have reduced enough of the noise from the diluted effect and captured enough signal.

Now, `B`'s conversion rate is only ``r percent(b_conversion_rate)`` because we are only counting the people who converted within 7-days from when they entered they experiment. But, it is a ``r percent(b_percent_increase_over_a)`` increase over `A`'s conversion rate of ``r percent(a_conversion_rate)``.

## What if we simulate p-value and converison rate over time, with and without the attribution window?

```{r}
set.seed(43)
traffic$day_entered_experiment <- floor(runif(n=num_trials, min=1, max=31))

traffic <- traffic %>%
    mutate(day_converted = ifelse(converted, day_entered_experiment + conversion_offset, NA),
           day_converted_within_window = ifelse(converted_within_window, day_entered_experiment + conversion_offset, NA))
```

```{r eval=FALSE, include=FALSE}
traffic %>%
    count(day_entered_experiment, variation) %>%
    ggplot(aes(x=day_entered_experiment, y=n, color=variation, group=variation)) +
    geom_line() +
    geom_point() +
    expand_limits(y=0)


traffic %>%
    mutate(offset_first_day = conversion_offset == 0) %>%
     count(day_entered_experiment, variation, offset_first_day) %>%
    filter(offset_first_day) %>%
    ggplot(aes(x=day_entered_experiment, y=n, color=variation, group=variation)) +
    geom_line() +
    geom_point() +
    expand_limits(y=0)
```

### No Attribution Window

```{r}
num_days_people_are_still_converting <- max(traffic$day_entered_experiment) + max(traffic$conversion_offset)
experiment <- data.frame(experiment_day = NULL, variation=NULL, conversions = NULL, trials = NULL)

for(ex_day in 1:num_days_people_are_still_converting) {
    day_results <- rbind(data.frame(experiment_day = ex_day,
                                    variation="A",
                                    cumulative_conversions = sum(traffic$variation == "A" & traffic$day_converted <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(traffic$variation == "A" & traffic$day_entered_experiment <= ex_day)),
                         data.frame(experiment_day = ex_day,
                                    variation="B",
                                    cumulative_conversions = sum(traffic$variation == "B" & traffic$day_converted <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(traffic$variation == "B" & traffic$day_entered_experiment <= ex_day)))

    experiment <- rbind(experiment, day_results)
}
experiment %>%
    mutate(cumulative_conversion_rate=cumulative_conversions / cumulative_trials) %>%
    ggplot(aes(x=experiment_day, y=cumulative_conversion_rate, color = variation, group=variation)) +
        geom_line() +
        geom_point() +
        scale_y_continuous(breaks = seq(0, 1, 0.05),
                           labels = percent_format()) +
        expand_limits(y=c(0,0.25)) +
        scale_x_continuous(breaks = seq(0, 100, 5)) +
        labs(title='Conversion rate over time - No Attribution Window',
         subtitle="This shows, for each day of the experiment (1-30) and each day after the experiment that people are still converting (31-60), the conversion rate if you we're to look a report on that day, using no attribution window", 
         y='Conversion Rate',
         x='Day of Experiment (and days after)',
         color='Variation') +
    theme_light()

temp <- experiment %>%
    #mutate(cumulative_conversion_rate=cumulative_conversions / cumulative_trials) %>%
  gather(variable, value, -(experiment_day:variation)) %>%
  unite(temp, variation, variable) %>%
  spread(temp, value)

prop.test_list <- with(temp, pmap(list(A_cumulative_conversions, B_cumulative_conversions, A_cumulative_trials, B_cumulative_trials),
                                    function(a, b, c, d) {
                                        if(a+b > 0) {
                                            prop.test(x=c(b, a), n=c(d, c))
                                        } else {
                                            NA    
                                        }
                                    }
                                    ))
temp$p_value <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$p.value   
    }
})
temp$b_minus_a <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         as.numeric(.$estimate[1] - .$estimate[2])
    }
})
temp$conf_low <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$conf.int[1]
    }
})
temp$conf_high <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$conf.int[2]
    }
})

temp %>%
    ggplot(aes(x=experiment_day, y=p_value)) +
        geom_line() +
        geom_point() +
    geom_text(aes(label=round(p_value, 3)), vjust=-0.5, check_overlap = TRUE) +
    geom_hline(yintercept = 0.05, color ='red') +
    scale_y_continuous(breaks = seq(0, 1, 0.05)) +
    expand_limits(y=c(0,1))

temp %>%
    ggplot(aes(x=experiment_day, y=b_minus_a)) +
        geom_line() +
        scale_y_continuous(labels = percent_format()) +
        geom_ribbon(aes(ymin = conf_low, ymax = conf_high), fill = 'green', alpha=0.15) +
        geom_ribbon(aes(ymin = ifelse(conf_low <= 0, conf_low, NA), ymax = ifelse(conf_low <= 0, conf_high, NA)), fill = 'red', alpha=0.45) +
        geom_ribbon(aes(ymin = ifelse(conf_low > 0, conf_low, NA), ymax = ifelse(conf_low > 0, conf_high, NA)), fill = 'green', alpha=0.2) +
        geom_hline(yintercept = 0, color='red', alpha=0.5) +
        geom_text(aes(label=percent(b_minus_a)), vjust=-1, check_overlap = TRUE) +
        labs(title='Difference in Conversion Rate of `B` - `A`, with Frequentist Confidence Interval - \nNo Attribution Window',
             y='Difference in Conversion Rate from B minus A',
             x='Day of Experiment (and days after)')

temp %>%
    mutate(a_cr = A_cumulative_conversions / A_cumulative_trials,
           b_minus_a = b_minus_a / a_cr,
           conf_low = conf_low / a_cr,
           conf_high = conf_high / a_cr) %>%
    ggplot(aes(x=experiment_day, y=b_minus_a)) +
        geom_line() +
        coord_cartesian(ylim=c(-0.10, 1)) +
        scale_y_continuous(labels = percent_format()) +
        geom_ribbon(aes(ymin = conf_low, ymax = conf_high), fill = 'green', alpha=0.15) +
        geom_ribbon(aes(ymin = ifelse(conf_low <= 0, conf_low, NA), ymax = ifelse(conf_low <= 0, conf_high, NA)), fill = 'red', alpha=0.45) +
        geom_ribbon(aes(ymin = ifelse(conf_low > 0, conf_low, NA), ymax = ifelse(conf_low > 0, conf_high, NA)), fill = 'green', alpha=0.2) +
        geom_hline(yintercept = 0, color='red', alpha=0.5) +
        geom_text(aes(label=percent(b_minus_a)), vjust=-1, check_overlap = TRUE) +    
        labs(title='Difference in Conversion Rate of `B` - `A`, with Frequentist Confidence Interval - \nNo Attribution Window',
             y='Lift (i.e. Percent change from A to B)',
             x='Day of Experiment (and days after)') 
```

```{r eval=FALSE, include=FALSE}
num_days_people_are_still_converting <- max(traffic$day_entered_experiment) + max(traffic$conversion_offset)
experiment <- data.frame(experiment_day = NULL, variation=NULL, conversions = NULL, trials = NULL)

for(ex_day in 1:num_days_people_are_still_converting) {
    local_traffic <- traffic %>% filter(day_entered_experiment + attribution_window_days < ex_day)

    day_results <- rbind(data.frame(experiment_day = ex_day,
                                    variation="A",
                                    cumulative_conversions = sum(local_traffic$variation == "A" & local_traffic$day_converted_within_window <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(local_traffic$variation == "A")),
                         data.frame(experiment_day = ex_day,
                                    variation="B",
                                    cumulative_conversions = sum(local_traffic$variation == "B" & local_traffic$day_converted_within_window <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(local_traffic$variation == "B")))

    experiment <- rbind(experiment, day_results)
}
experiment %>%
    mutate(cumulative_conversion_rate=cumulative_conversions / cumulative_trials) %>%
    ggplot(aes(x=experiment_day, y=cumulative_conversion_rate, color = variation, group=variation)) +
        geom_line() +
        geom_point() +
        scale_y_continuous(breaks = seq(0, 1, 0.05)) +
        expand_limits(y=c(0,0.3))

temp <- experiment %>%
    gather(variable, value, -(experiment_day:variation)) %>%
    unite(temp, variation, variable) %>%
    spread(temp, value)

prop.test_list <- with(temp, pmap(list(A_cumulative_conversions, B_cumulative_conversions, A_cumulative_trials, B_cumulative_trials),
                                    function(a, b, c, d) {
                                        if(a+b > 0) {
                                            prop.test(x=c(a, b), n=c(c, d))
                                        } else {
                                            NA    
                                        }
                                    }
                                    ))

temp$p_value <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$p.value   
    }
})
temp %>%
    ggplot(aes(x=experiment_day, y=p_value)) +
        geom_line() +
        geom_point() +
    geom_text(aes(label=round(p_value, 3)), vjust=-0.5, check_overlap = TRUE) +
    geom_hline(yintercept = 0.05, color ='red') +
    scale_y_continuous(breaks = seq(0, 1, 0.05)) +
    expand_limits(y=c(0,1))

```

### With Attribution Window

```{r}
num_days_people_are_still_converting <- max(traffic$day_entered_experiment) + max(traffic$conversion_offset) + attribution_window_days
experiment <- data.frame(experiment_day = NULL, variation=NULL, conversions = NULL, trials = NULL)

for(ex_day in 1:num_days_people_are_still_converting) {
    local_traffic <- traffic %>% filter(day_entered_experiment + attribution_window_days < ex_day)

    day_results <- rbind(data.frame(experiment_day = ex_day,
                                    variation="A",
                                    cumulative_conversions = sum(local_traffic$variation == "A" & local_traffic$day_converted_within_window <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(local_traffic$variation == "A")),
                         data.frame(experiment_day = ex_day,
                                    variation="B",
                                    cumulative_conversions = sum(local_traffic$variation == "B" & local_traffic$day_converted_within_window <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(local_traffic$variation == "B")))

    experiment <- rbind(experiment, day_results)
}

experiment %>%
    mutate(cumulative_conversion_rate=cumulative_conversions / cumulative_trials) %>%
    ggplot(aes(x=experiment_day, y=cumulative_conversion_rate, color = variation, group=variation)) +
        geom_line() +
        geom_point() +
        scale_y_continuous(breaks = seq(0, 1, 0.05),
                           labels = percent_format()) +
        expand_limits(y=c(0,0.25)) +
        scale_x_continuous(breaks = seq(0, 100, 5)) +
        labs(title='Conversion rate over time - With Attribution Window',
         subtitle="This shows, for each day of the experiment (1-30) and each day after the experiment that people are still converting (31-60), the conversion rate if you we're to look a report on that day, using With attribution window", 
         y='Conversion Rate',
         x='Day of Experiment (and days after)',
         color='Variation') +
    theme_light()

temp <- experiment %>%
    #mutate(cumulative_conversion_rate=cumulative_conversions / cumulative_trials) %>%
  gather(variable, value, -(experiment_day:variation)) %>%
  unite(temp, variation, variable) %>%
  spread(temp, value)

prop.test_list <- with(temp, pmap(list(A_cumulative_conversions, B_cumulative_conversions, A_cumulative_trials, B_cumulative_trials),
                                    function(a, b, c, d) {
                                        if(a+b > 0) {
                                            prop.test(x=c(b, a), n=c(d, c))
                                        } else {
                                            NA    
                                        }
                                    }
                                    ))
temp$p_value <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$p.value   
    }
})
temp$b_minus_a <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         as.numeric(.$estimate[1] - .$estimate[2])
    }
})
temp$conf_low <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$conf.int[1]
    }
})
temp$conf_high <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$conf.int[2]
    }
})

temp %>%
    ggplot(aes(x=experiment_day, y=p_value)) +
        geom_line() +
        geom_point() +
    geom_text(aes(label=round(p_value, 3)), vjust=-0.5, check_overlap = TRUE) +
    geom_hline(yintercept = 0.05, color ='red') +
    scale_y_continuous(breaks = seq(0, 1, 0.05)) +
    expand_limits(y=c(0,1))

temp %>%
    ggplot(aes(x=experiment_day, y=b_minus_a)) +
        geom_line() +
        scale_y_continuous(labels = percent_format()) +
        geom_ribbon(aes(ymin = conf_low, ymax = conf_high), fill = 'green', alpha=0.15) +
        geom_ribbon(aes(ymin = ifelse(conf_low <= 0, conf_low, NA), ymax = ifelse(conf_low <= 0, conf_high, NA)), fill = 'red', alpha=0.45) +
        geom_ribbon(aes(ymin = ifelse(conf_low > 0, conf_low, NA), ymax = ifelse(conf_low > 0, conf_high, NA)), fill = 'green', alpha=0.2) +
        geom_hline(yintercept = 0, color='red', alpha=0.5) +
        geom_text(aes(label=percent(b_minus_a)), vjust=-1, check_overlap = TRUE) +
        labs(title='Difference in Conversion Rate of `B` - `A`, with Frequentist Confidence Interval - \nWith Attribution Window',
             y='Difference in Conversion Rate from B minus A',
             x='Day of Experiment (and days after)')

temp %>%
    mutate(a_cr = A_cumulative_conversions / A_cumulative_trials,
           b_minus_a = b_minus_a / a_cr,
           conf_low = conf_low / a_cr,
           conf_high = conf_high / a_cr) %>%
    ggplot(aes(x=experiment_day, y=b_minus_a)) +
        geom_line() +
        coord_cartesian(ylim=c(-0.10, 1)) +
        scale_y_continuous(labels = percent_format()) +
        geom_ribbon(aes(ymin = conf_low, ymax = conf_high), fill = 'green', alpha=0.15) +
        geom_ribbon(aes(ymin = ifelse(conf_low <= 0, conf_low, NA), ymax = ifelse(conf_low <= 0, conf_high, NA)), fill = 'red', alpha=0.45) +
        geom_ribbon(aes(ymin = ifelse(conf_low > 0, conf_low, NA), ymax = ifelse(conf_low > 0, conf_high, NA)), fill = 'green', alpha=0.2) +
        geom_hline(yintercept = 0, color='red', alpha=0.5) +
        geom_text(aes(label=percent(b_minus_a)), vjust=-1, check_overlap = TRUE) +    
        labs(title='Difference in Conversion Rate of `B` - `A`, with Frequentist Confidence Interval - \nWith Attribution Window',
             y='Lift (i.e. Percent change from A to B)',
             x='Day of Experiment (and days after)') 
```


# Cautions

- The effects seen from the diminishing lift and the attribution windows are based on many assumptions, including the distribution of days from entering into the experiment to conerting, and the rate/distribution of diminishing effect over time. There is also random variation in the simulation. It's possible that in other cases, other types of random variation will not pick up the same patterns.







































# Thoughts

- Who is an A/B test inherently affecting? Are A/B tests affecting people that are more susceptible to certain psychological manipulations? Do the effects of A/B tests affect  people that take longer to make a decision?