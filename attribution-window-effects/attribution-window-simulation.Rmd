---
title: "README"
author: "Shane Kercheval"
output:
    md_document:
    variant: markdown_github
toc: true
toc_depth: 4
---
    
```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(knitr)
library(tidyverse)
library(lubridate)
library(scales)
options(scipen=999) # non-scientific notation

knitr::opts_chunk$set(echo = FALSE)
```

# A/B Tests, Diminishing Lift, & Attribution Windows

A few definitions:

- `control`: refers to either the `A` group, or the thing you show to the `A` group
- `variant`: refers to either the `B` group, or the thing you show to the `B` group
- `variation`: refers to either the `A` (`control`) group or `B` (`variant`).
- `lift`: the percent increase in the conversion rate (or the probability a random person will convert) from the `A` group to the `B` based on the thing you are testing in the `B` group (i.e. the `variant`)
- `immediate lift`: the lift experienced among a subpopulation of people who convert in a relatively short amount of time, relative from when the variant is shown.
- `diminishing lift`: the potential phenomenon where, the longer it takes people to convert relative from when the variant was seen, the less lift the group will experience from the variant.
- `attribution window` the maximum number of days you allow conversions to be attributed to the corresponding variation based on when the person entered the experiment. So, person `x` enters the experiment on day `y`. With a `7-day` attribution window, we would only count `x`'s conversion if the conversion event took place within `7` days after `y`. This would be the same for both variations.

## There are two questions when running A/B tests that don't seem to be discussed much.

Let's say you're running an A/B test and the variant is outperforming the control. My assumption is that, despite the overall outperformance (i.e. the "lift") of the variant, the lift is highest in the first moments that it is seen by each person, and then decreases over time, such that the longer it takes the person to convert, the less impact that the variant had (relative to the control) on their conversion decision. In other words, the variant has provided an **immediate lift** but the lift diminishes over time, such that the people in the `A` and `B` groups who are converting sometime in the future (say, at the extreme, a year after they saw the `control` and `variation`, respectively) are converting at virtually the same conversion rates.(++) We can probably make similar statements for (perhaps) days, (mostly likely) weeks, and (certainly) months after the variation is seen.

If true, obviously the rate (or distribution of) of diminishing lift and the time-period it takes to completely diminish depends on the product/experiment/target-market/etc..

The concern is that, if a sub-population in our experiment (belonging to both the control group and the variant group) are converting at the same rates, **despite any immediate lift the variant has**, then when look at the results of the A/B test, those conversions are diluting, or masking, the immediate lift that the variant gives, and we might conclude that the variant has little or no effect. So, a contrived example would be: for a given experiment, the variant has a 7% statistically significant *immediate lift* (i.e. for the people who convert within, say, 2 days, the variant out-performs the control by a relative 7% increase in the conversion rate) but when looking at the entire population (let's say they have all been given a month to convert, and we analyze the results a month after the experiment ends), the lift is only 3% and not statistically significant. Is this type of contrived example realistic under some reasonable assumptions?+++

So, the first question is: **How does the diminishing lift of the variant, over time, affect the outcome of the experiment?**

In the contrived example above, I implied that there was a 7% lift detected if we used an attribution window of 2 days but only a 3% lift detected if we used a 30-day attribution window. So, in the former case we only counted conversions that happened within 2 days from the time a given person saw the experiment to when they converted. In the latter case, we counted conversions that happened within 30 days (from when the experiment was seen to the conversion date). The other assumption implied above was that this decrease was from the fact that, when grouping people by the amount of days it takes them to convert, the conversion rates of both groups start to converge as the lift from the variant diminishes. And the more conversions that we count that in the variant that have diminished lift, the more diluted the results became.++++

But, the typical process for A/B testing doesn't seem to consider/use an attribution windows at all: You run a test for, say, 30 days (depending on your sample size calculation). The people who enter the experiment on the first day it started get 30 days to convert (i.e. their conversion event is counted at any point during the experiment). The people who enter into the experiment on the last day get <= 1 day to convert. Perhaps some teams give an extra, say, 7 days for conversion (so people who see the experiment on the first day, get 37 days to convert, people who see it on the last day get 7 days.).

This introduces the next question: **What effect does an attribution window have on the results of an experiment?**

We'll answer that question by making some reasonable assumptions about the distributions of how long it takes people to convert and the diminishing lift over time. It will be a contrived example, but the purpose is **not** to show that the concerns from above and results from below will show up in *every* A/B test, it is only to show what is *possible* under *reasonable* assumptions.

---

++*Perhaps this means that A/B testing is inherently targeting people who either A) more susceptible to psychological manipulation (e.g. mind hacks, high pressure sales tactics, buying things based on whether it's sunny or dark outside, etc.) and/or B) wanting to make an immediately decision and/or C) impulsive buyers. At the very least, it seems like the target segment of A/B tests is the subpopulation of the target market that takes less time to make decisions and/or has a more immediate need. But, that's a direction I don't want to go right now. The caveat to this is that, there are of course experiments that you run where, for example, you've completely redesigned your product and that new experience does have a long-lasting impact on the customers' decision. In those cases, the lift is high enough that any concerns we deal with here are probably going to disappear from the large lift. I assume that's not the case for most A/B tests.*

+++*It’s worth noting that, for this example, the overall lift of the variant on the entire population is, indeed, only 3%. My point is not that `3%` is inaccurate, but that it includes people that, for an abundance of possible reasons, aren’t as influenced by the variant compared with others. So, are you trying to detect a 3% change to the entire population, or a 7% change of the population that is inherently more affected by A/B tests?*

++++*An issue rated to note(+++) is the required sample size (and, therefore, duration) of the experiment in order to detect the lift, if it exists. There are two relevant trade-offs. One trade-off is that using a smaller attribution window leads to lower conversion rates (i.e. you are only counting a subset of conversions). All other things considered, a lower conversion rate requires a larger sample size. So, detecting a 5% change with a baseline conversion rate of 20% takes longer to test than with a baseline conversion rate of 30%. On the other hand, if you capture enough signal in the immediate lift (again, if it exists) and reduce a lot of the noise from the diminishing lift, a smaller attribution window will increase the lift found. So for example, the sample size required to detect a 3% change (from the control to the variant) is a lot larger than the sample size required to detect a 7% change. Therefore, in order to use attribution windows, the assumption is that these tradeoffs have to either balance out, or at least don’t affect the feasibility of running the A/B test.*

---

## Simulate A/B Groups

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
seed_traffic <- 1
seed_conversion <- 2
seed_day_entered_experiment <- 3


# seed_traffic <- 63
# seed_conversion <- 54
# seed_day_entered_experiment <- 42

num_trials <- 120000
# used to simulate what would happen if we were to continue running the experiment after it is shut
maximum_experiment_duration <- 90
actual_experiment_duration <- 60
trials_per_day <- num_trials / maximum_experiment_duration
baseline_conversion_rate <- 0.10
simulated_percent_increase_in_b <- 0.07
variant_conversion_rate <- baseline_conversion_rate + (baseline_conversion_rate * simulated_percent_increase_in_b)

required_sample_size <- ceiling(power.prop.test(p1=baseline_conversion_rate,
                                    p2=variant_conversion_rate,
                                    power=0.8,
                                    sig.level=0.05)$n*2)

required_days <- required_sample_size / trials_per_day
```

Let's simulate a `r actual_experiment_duration`-day experiment where ~``r comma_format()(round(trials_per_day))`` people enter the experiment daily, giving a total of about ``r comma_format()(round(actual_experiment_duration * trials_per_day))`` are part of the experiment after `r actual_experiment_duration` days.

Then we'll randomly assign people into group `A` or group `B`, based on a random number generator, which will be roughly ``r comma_format()((round(actual_experiment_duration * trials_per_day)) / 2)`` per randomized variation.

```{r create_traffic}
set.seed(seed_traffic)
traffic <- data.frame(id=sample(1:10000000, num_trials, replace=F))

set.seed(seed_day_entered_experiment)
traffic$day_entered_experiment <- floor(runif(n=num_trials, min=1, max=maximum_experiment_duration))

set.seed(seed_traffic)
traffic$variation <- ifelse(rbinom(num_trials, 1, 0.5), 'A', 'B')

traffic %>% 
    filter(day_entered_experiment <= actual_experiment_duration) %>%
    count(variation) %>%
    ggplot(aes(x=variation, y=n, fill=variation)) +
    geom_col() +
    geom_text(aes(label=comma_format()(n)), vjust = -0.4) +
    theme_light() +
    theme(legend.position = 'none') +
    scale_y_continuous(labels = comma_format()) +
    scale_fill_manual("legend", values = c("A" = "#FF9500", "B" = "#19A6FF")) +
    labs(title='Number of people (trials) in each variation',
         y='Trials',
         x='Variation')
```

```{r create_conversion_rate}
traffic$baseline_conversion_rate <- baseline_conversion_rate
```

## Simulate Conversion Rates

### Simulate No Change

Now, we'll give everyone a baseline conversion rate of ``r percent(baseline_conversion_rate)``.

Specifically, we'll assign each person a conversion rate (i.e. probability), then take a random sample from
the binomial distribution using their probability to determine if that person converted.

Let's simulate the conversion for each person and see the overall conversion rates for each variation.

```{r create_conversion_rate2}
converted_by_id <- function(id, conversion_rate) {
    #set.seed(id)
    return (as.logical(rbinom(1, 1, conversion_rate)))
}
set.seed(seed_conversion)
traffic$converted <- map2_lgl(traffic$id, traffic$baseline_conversion_rate, ~ converted_by_id(.x, .y))

experiment_traffic <- traffic %>% filter(day_entered_experiment <= actual_experiment_duration)
conversion_table <- table(experiment_traffic$variation, experiment_traffic$converted)
conversions <- c(conversion_table['A', 'TRUE'], conversion_table['B', 'TRUE'])
non_conversions <- c(conversion_table['A', 'FALSE'], conversion_table['B', 'FALSE'])
trials <- conversions + non_conversions

experiment_traffic %>% 
    count(variation, converted) %>%
    group_by(variation) %>%
    mutate(baseline_conversion_rate = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=variation, y=n, fill=converted)) +
    scale_y_continuous(labels = comma_format()) +
    geom_col(position='dodge') +
    scale_fill_manual(values = c("TRUE" = "#85CD75", "FALSE" = "#F57670")) +
    geom_text(aes(label=comma_format()(n)),
              position = position_dodge(width=0.9),
              vjust = 1.2) +
    geom_text(aes(label=percent_format()(baseline_conversion_rate)),
              position = position_dodge(width=0.9),
              vjust = -0.4) +
    labs(title=paste('Simulated', percent(baseline_conversion_rate),'(Baseline) Conversion Rate from Binomial Distribution'),
         y='Trials',
         x='Variation',
         fill='Converted') +
    theme_light()
```

We can see that the conversion rate for each group is roughly the same. Using the Chi-Square test for proportions, we get a p-value of ``r prop.test(x=conversions, n=trials)$p.value``, so there is no statistical difference between the two conversion rates.

So far, we've simulated an experiment in which there was **no** change in the `A` group vs the `B` group. It was an experiment that is equivalent to an `A`/`A` test.

### Simulate ``r percent(simulated_percent_increase_in_b)`` Increase in Group B

Now, let's simulate everyone in the `B` group having a ``r percent(simulated_percent_increase_in_b)`` increase (due from the effects of the new variant we are testing out.)

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
traffic <- traffic %>%
    mutate(lift = ifelse(variation == 'A', 0, simulated_percent_increase_in_b)) %>%
    mutate(conversion_rate = baseline_conversion_rate + (baseline_conversion_rate * lift)) %>%
    select(-converted)

set.seed(seed_conversion)
converted_no_diminishing_effects <- map2_lgl(traffic$id, traffic$conversion_rate, ~ converted_by_id(.x, .y))
traffic$converted <- converted_no_diminishing_effects

experiment_traffic <- traffic %>% filter(day_entered_experiment <= actual_experiment_duration)

conversion_table <- table(experiment_traffic$variation, experiment_traffic$converted)
conversions <- c(conversion_table['A', 'TRUE'], conversion_table['B', 'TRUE'])
non_conversions <- c(conversion_table['A', 'FALSE'], conversion_table['B', 'FALSE'])
trials <- conversions + non_conversions

a_conversion_rate <- conversions[1] / trials[1]
b_conversion_rate <- conversions[2] / trials[2]

b_percent_increase_over_baseline <- (b_conversion_rate - baseline_conversion_rate) / b_conversion_rate
b_percent_increase_over_a <- (b_conversion_rate - a_conversion_rate) / b_conversion_rate

experiment_traffic %>% 
    count(variation, converted) %>%
    group_by(variation) %>%
    mutate(conversion_rate = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=variation, y=n, fill=converted)) +
    geom_col(position='dodge') +
    scale_y_continuous(labels = comma_format()) +
    scale_fill_manual(values = c("TRUE" = "#85CD75", "FALSE" = "#F57670")) +
    geom_text(aes(label=comma_format()(n)),
              position = position_dodge(width=0.9),
              vjust = 1.2) +
    geom_text(aes(label=percent_format()(conversion_rate)),
              position = position_dodge(width=0.9),
              vjust = -0.4) +
    labs(title=paste('Simulated', percent(baseline_conversion_rate),'Baseline Conversion Rate from Binomial Distribution,\nwith', percent(simulated_percent_increase_in_b),'Lift from Effects of B variation.'),
         y='Trials',
         x='Variation',
         fill='Converted') +
    theme_light()
```

With the new conversion rate (and due to random variation i.e. random sampling from binomial distribution), `B`'s conversion rate of ``r percent(b_conversion_rate)`` is a ``r percent(b_percent_increase_over_baseline)`` increase over the **true** baseline conversion rate of ``r percent(baseline_conversion_rate)``, and a ``r percent(b_percent_increase_over_a)`` increase over `A`'s conversion rate of ``r percent(a_conversion_rate)``.

The P-value is now ``r prop.test(x=conversions, n=trials)$p.value`` and there is a statistically significant difference between the variations.

## But, here's the problem.

These conversion rates assume that everyone who has converted, have been given any amount of time to convert, **and regardless of how long it took them to convert (relative to when they entered the experiment), they had the same lift from the experiment** (specifically, everyone in the `B` group had a ``r percent(baseline_conversion_rate + (baseline_conversion_rate * simulated_percent_increase_in_b))`` probability of converting (``r percent(baseline_conversion_rate)`` + (``r percent(baseline_conversion_rate)`` * ``r percent(simulated_percent_increase_in_b)``)), which was used to pull a random sample from the binomial distribution to determine if they converted).

This doesn't consider:

- the fact that people take different lengths of time to convert (relative to when they entered into the experiment) and 
- **the effects that that the A/B test has on a person, or group of people, diminishes over time.** It cannot be assumed that the effect of seeing a certain button, or certain design, or a certain experience, will have the same effect in that instant vs. 1 hour later vs. 1 day later vs. 30 days later. The effect of that "thing" on a person's decision or action will almost always, for almost everyone, diminish over time.

So, let's answer the first question from above: **How does the diminishing lift of the variant, over time, affect the outcome of the experiment?**

We have to do 2 things.

First, we have to **simulate people converting at different lengths of time, relative to when they saw the experiment**.

Second, we have to **simulate the effect of the experiment diminishing over time**. 

### Simulate the Number of Days from Entering the Experiment to Converting

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
scale_a_b <- function(x, a, b) {

    (b - a) * ((x - min(x)) / (max(x) - min(x))) + a 
}

set.seed(44)
#offset <- round(scale_a_b(rlnorm(n=num_trials, meanlog = 0.04), 0, 50))
#hist(offset)
offset <- round(scale_a_b(rgamma(num_trials, shape = 1.7), 1, 75))
offset <- ifelse(offset <= 1, 2, offset)
offset <- offset - 2

traffic <- traffic %>%
    mutate(conversion_offset = offset,
           #conversion_offset = ifelse(conversion_offset > 30, round(runif(1, min=20, max=30)), conversion_offset),
           conversion_offset = ifelse(conversion_offset < 0, 0, conversion_offset))

traffic$conversion_offset <- map_dbl(traffic$conversion_offset, ~ {
    if(. > 30) {
        return (round(runif(1, min=20, max=30)))
    } else {
        return (.)
    }
})

traffic %>%
    group_by(conversion_offset, variation) %>%
    summarise(num_trials=n(),
              average_conversion_rate = mean(conversion_rate)) %>%
    ggplot(aes(x=conversion_offset, y=num_trials, fill=average_conversion_rate)) +
    geom_col() +
    scale_y_continuous(labels = comma_format()) +
    scale_x_continuous(breaks = seq(0, 100)) +
    scale_fill_gradient(labels = percent) +
    facet_wrap(~variation, nrow=2) +
    labs(title='Simulated Distribution of Days from Entering Experiment to Conversion.',
         subtitle = 'This shows the number of people randomly assigned to various days;\nthey will convert at the conversion rate shown in the legend. ',
         y='Number of people/trials',
         x='Number of Days from Entering Experiment to Conversion',
         fill='Conversion Rate')
```

Everyone in the simulation is assigned a random number (based on a modified Gamma distribution) that represents the number of days that will take to convert **if they convert**. They are assigned the number before the binomial distribution determines if they convert. The reason for this is technical and intuitive. The technical reason is that the probability given to the binomial distribution will be based on the final probability assigned to the person which will be based on the number of days they are assigned and the diminishing effect calculated below. One intuitive reason (maybe a justification), is that this number incorporates a lot of assumptions and characteristics about each person that we can't directly simulate: the buying impulse of the person, the degree of immediate need, the person's general thoughtfulness and scrutiny of making a decision (e.g. purchase), susceptibility to psychological "growth hacks", indecisiveness, and so on. These characteristics affect how long it will take them to convert and, either, as a result, or in addition to, how much they will be influenced by the variant.

### Simulate the the Diminishing Effect of the Variant Over Time.

Now, let's simulate a diminishing effect that the variant has on a person, based on the number of days it takes them to convert (if they do convert).

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
stand_dev <- .5
scale_norm <- 1 / dnorm(0, sd=stand_dev)

relative_effect <- function(day_number) {
    return (dnorm(((day_number) / 30 * 2), sd=stand_dev) * scale_norm)
}

data.frame(days_from_experiment_to_conversion=0:30) %>%
    mutate(relative_effect = relative_effect(days_from_experiment_to_conversion)) %>%
    ggplot(aes(x=days_from_experiment_to_conversion, y=relative_effect)) +
    geom_line() +
    geom_point() +
    geom_text(aes(label=percent(relative_effect)), vjust=-0.5, check_overlap = TRUE) +
    scale_x_continuous(breaks = 0:30) +
    labs(title='Percent of Lift Applied to the Days From Entering Experiment to Conversion.',
         subtitle='The Lift will be multiplied by this percentage for the corresponding day of conversion.',
         y='Relative Effect (Lift * y)',
         x='Number of Days from Experiment to Conversion Event.')
```

Here I'm simply taking the second half of a normal distribution (i.e. bell-curve) and modifying it to go from 100% to ~0% over 30 days, which simulates the "relative effect" of the Lift of the experiment over time.

So, for example, if a person in the `B` group is assigned (from above) a value that indicates they will potentially convert in the first day (remember, there conversion depending on a random draw from the binomial distribution), then that person will get the full effect of the lift of the variant (i.e. they will get `100%` (top of the curve) of the ``r percent(simulated_percent_increase_in_b)`` lift). On the other hand, if the person converts 30 days after seeing the experiment, they experience very little lift from the experiment (the lift is not quite `0%`, but close.)

The rate of the diminishing effect of an experiment is large assumption of this simulation. This is my best guess at a reasonable rate, and depending on the product/service/experiment, this graph will certainly change.

Now let's apply the "Percent of Lift Applied" to the Baseline Conversion Rate and the Lift in the Variant.

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
traffic <- traffic %>%
    mutate(diminished_conversion_rate = baseline_conversion_rate + (baseline_conversion_rate * lift * relative_effect(conversion_offset)))
# %>%
#     select(-converted)

stopifnot(all(ifelse(traffic$variation == "A", traffic$baseline_conversion_rate == traffic$conversion_rate, TRUE)))
stopifnot(all(ifelse(traffic$variation == "A", traffic$baseline_conversion_rate == traffic$diminished_conversion_rate, TRUE)))

traffic %>%
    # filter()
    # filter(!is.na(conversion_offset)) %>%
    group_by(conversion_offset, variation) %>%
    summarise(diminished_conversion_rate = mean(diminished_conversion_rate)) %>%
 #   spread(variation, diminished_conversion_rate) %>%
    ggplot(aes(x=conversion_offset, y=diminished_conversion_rate, color=variation)) +
    geom_line() +
    geom_point() +
    geom_text(aes(label=percent_format()(diminished_conversion_rate)), check_overlap=TRUE, size = 3, vjust=-0.5) + 
    scale_y_continuous(labels = percent_format()) +
    scale_color_manual(values = c("A" = "#FF9500", "B" = "#19A6FF")) +
    #expand_limits(y=0) +
    labs(title="Diminishing Effect of Variant",
         y='Conversion Rate',
         x='Days from Entering Experiment to Conversion',
         color='Variation')
```

This graph shows the conversion rate (i.e. probability used in the random draw from the binomial distribution) for each person according to the number of days it took them to convert, relative to when they entered the experiment.

Again, people who will potentially convert in the first days of joining the experiment are assigned a conversion rate associated with majority of the lift, and people who take a long time to convert, convert at the same rate as the baseline conversion rate (which is the same rate as the `A` group).

We can take our graph from above and show how the diminished effect influences the conversion rate of the people assigned to various days.

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
traffic %>%
    group_by(conversion_offset, variation) %>%
    summarise(num_trials=n(),
              average_conversion_rate = mean(diminished_conversion_rate)) %>%
    ggplot(aes(x=conversion_offset, y=num_trials, fill=average_conversion_rate)) +
    geom_col() +
    scale_y_continuous(labels = comma_format()) +
    scale_x_continuous(breaks = seq(0, 100)) +
    scale_fill_gradient(labels = percent) +
    facet_wrap(~variation, nrow=2) +
    labs(title='Simulated Distribution of Days from Entering Experiment to Conversion.',
         subtitle = 'This shows the number of people randomly assigned to various days;\nthey will convert at the conversion rate shown in the legend.\nThe longer it takes a person in the B group to convert, the less Lift they receive from the variant.',
         y='Number of people/trials',
         x='Number of Days from Entering Experiment to Conversion',
         fill='Conversion Rate')
```

**This is the key point**. The longer it takes people to convert, the less **lift** there is, and the more of these conversions you allow into your results, the more it will mask the immediate lift. The "immediate lift" is symbolized in the graph above with a light blue color in the `B` group.

No such effect exists in the `A` group because they maintain the baseline conversion rate (there is no lift and nothing to diminish).

**The people who convert >= ~15 days from when they saw the variant, start to become much less affected by the variant and start to convert at the same rates as the control group.**

Another way of saying this is it's likely that the people who converted after 30 days were going to do so anyway, with or without the thing you tested (or probably many of the nuances and brain hacks in the website).

## Update Conversion Rates, with Diminishing Lift

So, let's look at the results of the A/B test with the updated conversion rates.

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
set.seed(seed_conversion)
new_converted <- map2_lgl(traffic$id, traffic$diminished_conversion_rate, ~ converted_by_id(.x, .y))

# ensure that conversions of variation A don't change
stopifnot(all(ifelse(traffic$variation == "A", converted_no_diminishing_effects == new_converted, TRUE)))

traffic$converted <- new_converted

experiment_traffic <- traffic %>% filter(day_entered_experiment <= actual_experiment_duration)

conversion_table <- table(experiment_traffic$variation, experiment_traffic$converted)
conversions <- c(conversion_table['A', 'TRUE'], conversion_table['B', 'TRUE'])
non_conversions <- c(conversion_table['A', 'FALSE'], conversion_table['B', 'FALSE'])
trials <- conversions + non_conversions

a_conversion_rate <- conversions[1] / trials[1]
b_conversion_rate <- conversions[2] / trials[2]

b_percent_increase_over_baseline <- (b_conversion_rate - baseline_conversion_rate) / b_conversion_rate
b_percent_increase_over_a <- (b_conversion_rate - a_conversion_rate) / b_conversion_rate

experiment_traffic %>% 
    count(variation, converted) %>%
    group_by(variation) %>%
    mutate(conversion_rate = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=variation, y=n, fill=converted)) +
    geom_col(position='dodge') +
    scale_y_continuous(labels = comma_format()) +
    scale_fill_manual(values = c("TRUE" = "#85CD75", "FALSE" = "#F57670")) +
    geom_text(aes(label=comma_format()(n)),
              position = position_dodge(width=0.9),
              vjust = 1.2) +
    geom_text(aes(label=percent_format()(conversion_rate)),
              position = position_dodge(width=0.9),
              vjust = -0.4) +
    labs(title=paste('Simulated', percent(baseline_conversion_rate),'Baseline Conversion Rate from Binomial Distribution,\nwith (diminishing)', percent(simulated_percent_increase_in_b),'Lift from Effects of B variation.'),
         subtitle='This is the conversion rate after simulated the effect of diminishing Lift over time.',
         y='Trials',
         x='Variation',
         fill='Converted') +
    theme_light()
```

Including the people in the experiment that converted many days after they saw the experiment, seems to dilute the conversion rate. Again, more and more people are converting at the same conversion rate as the `A` group.

Now, `B`'s conversion rate of ``r percent(b_conversion_rate)`` is only a ``r percent(b_percent_increase_over_baseline)`` increase over the **true** baseline conversion rate of ``r percent(baseline_conversion_rate)``, and a ``r percent(b_percent_increase_over_a)`` increase over `A`'s conversion rate of ``r percent(a_conversion_rate)``, which hasn't changed since the last simulation.

The P-value is no longer statistically significant: ``r prop.test(x=conversions, n=trials)$p.value``

But, in truth, what we did above isn't exactly the same as running a test, shutting it off, and then analyzing the results. I ran the experiment for 30 days, but still gave people unlimited time to convert, as I haven't brought in the effects of attribution windows yet. So, let's examine what the p-values and lift look like over-time. We'll simulate running the experiment for 30 days (as I did above), but we'll allow an additional 14 days after the experiment for everyone to convert (so the people entering the experiment get 14 days). This is actually forcing an attribution window of varying degrees. It forces a 14-day attribution window for the people who entered the experiment on the last day, an 15-day attribution window for the people that enter the day before, and so on. I chose 14 days because I want to limit the effects of the attribution-window-like effect you would see if I chose a 2-day or 7-day window, like I will below. 

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
number_of_days_after_experiment_to_allow <- 14

p_value_traffic <- traffic %>%
    mutate(day_converted = ifelse(converted, day_entered_experiment + conversion_offset, NA))# %>%

num_days_people_are_still_converting <- max(p_value_traffic$day_entered_experiment) + max(p_value_traffic$conversion_offset)
experiment <- data.frame(experiment_day = NULL, variation=NULL, conversions = NULL, trials = NULL)

for(ex_day in 1:num_days_people_are_still_converting) {
    day_results <- rbind(data.frame(experiment_day = ex_day,
                                    variation="A",
                                    cumulative_conversions = sum(p_value_traffic$variation == "A" & p_value_traffic$day_converted <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(p_value_traffic$variation == "A" & p_value_traffic$day_entered_experiment <= ex_day)),
                         data.frame(experiment_day = ex_day,
                                    variation="B",
                                    cumulative_conversions = sum(p_value_traffic$variation == "B" & p_value_traffic$day_converted <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(p_value_traffic$variation == "B" & p_value_traffic$day_entered_experiment <= ex_day)))

    experiment <- rbind(experiment, day_results)
}
```

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
experiment %>%
    mutate(cumulative_conversion_rate=cumulative_conversions / cumulative_trials) %>%
    ggplot(aes(x=experiment_day, y=cumulative_conversion_rate, color = variation, group=variation)) +
        geom_line() +
        geom_point() +
        geom_vline(xintercept = actual_experiment_duration, color='orange', alpha=0.5, linetype="dashed", size=1.5) + 
        geom_vline(xintercept = maximum_experiment_duration, color='purple', alpha=0.5, linetype="dashed", size=1.5) + 
        #geom_vline(xintercept = actual_experiment_duration, color='red', alpha=0.5) + 
        scale_y_continuous(breaks = seq(0, 1, 0.05),
                           labels = percent_format()) +
        expand_limits(y=c(0,0.12)) +
        scale_x_continuous(breaks = seq(0, 200, 5)) +
        scale_color_manual(values = c("A" = "#FF9500", "B" = "#19A6FF")) +
        labs(title='Conversion rate over time - No Attribution Window',
         subtitle='Orange Dashed Line Represents End of Experiment at 60 days.\nPurple Dashed Line Represents End of Experiment at 90 Days.',
         y='Conversion Rate',
         x='Day of Experiment (and days after)',
         color='Variation') +
        theme_light()
```

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
cumulative_experiment <- experiment %>%
    gather(variable, value, -(experiment_day:variation)) %>%
    unite(cumulative_experiment, variation, variable) %>%
    spread(cumulative_experiment, value)

prop.test_list <- with(cumulative_experiment, pmap(list(A_cumulative_conversions, B_cumulative_conversions, A_cumulative_trials, B_cumulative_trials),
                                    function(a, b, c, d) {
                                        if(a+b > 0) {
                                            prop.test(x=c(b, a), n=c(d, c))
                                        } else {
                                            NA    
                                        }
                                    }
                                    ))
cumulative_experiment$p_value <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$p.value   
    }
})
cumulative_experiment$b_minus_a <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         as.numeric(.$estimate[1] - .$estimate[2])
    }
})
cumulative_experiment$conf_low <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$conf.int[1]
    }
})

cumulative_experiment$conf_high <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$conf.int[2]
    }
})

cumulative_experiment %>%
    ggplot(aes(x=experiment_day, y=p_value)) +
        geom_line() +
        geom_point() +
        geom_text(aes(label=round(p_value, 3)), vjust=-0.5, check_overlap = TRUE) +
        geom_hline(yintercept = 0.05, color ='red', alpha=0.5, size=1.5) +
        scale_x_continuous(breaks = seq(1, 200, 5)) +
        scale_y_continuous(breaks = seq(0, 1, 0.05)) +
        expand_limits(y=c(0,1)) +
        geom_vline(xintercept = actual_experiment_duration, color='orange', alpha=0.5, linetype="dashed", size=1.5) + 
        geom_vline(xintercept = maximum_experiment_duration, color='purple', alpha=0.5, linetype="dashed", size=1.5) + 
        labs(title='P-value over time - No Attribution Window',
            subtitle='Orange Dashed Line Represents End of Experiment at 60 days.\nPurple Dashed Line Represents End of Experiment at 90 Days.',
            y='Conversion Rate',
            x='Day of Experiment (and days after)',
            color='Variation') +
        theme_light()
```

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
# cumulative_experiment %>%
#     ggplot(aes(x=experiment_day, y=b_minus_a)) +
#         geom_line() +
#         scale_y_continuous(labels = percent_format()) +
#         geom_ribbon(aes(ymin = conf_low, ymax = conf_high), fill = 'green', alpha=0.15) +
#         geom_ribbon(aes(ymin = ifelse(conf_low <= 0, conf_low, NA), ymax = ifelse(conf_low <= 0, conf_high, NA)), fill = 'red', alpha=0.45) +
#         geom_ribbon(aes(ymin = ifelse(conf_low > 0, conf_low, NA), ymax = ifelse(conf_low > 0, conf_high, NA)), fill = 'green', alpha=0.2) +
#         geom_hline(yintercept = 0, color='red', alpha=0.5, size=1.5) +
#         geom_vline(xintercept = actual_experiment_duration + number_of_days_after_experiment_to_allow, color ='red') +
#         geom_text(aes(label=percent(b_minus_a)), vjust=-1, check_overlap = TRUE) +
#         labs(title='Difference in Conversion Rate of `B` - `A`, with Frequentist Confidence Interval - \nNo Attribution Window',
#              y='Difference in Conversion Rate from B minus A',
#              x='Day of Experiment (and days after)')

cumulative_experiment %>%
    mutate(a_cr = A_cumulative_conversions / A_cumulative_trials,
           b_minus_a = b_minus_a / a_cr,
           conf_low = conf_low / a_cr,
           conf_high = conf_high / a_cr) %>%
    ggplot(aes(x=experiment_day, y=b_minus_a)) +
        geom_line() +
        coord_cartesian(ylim=c(-0.10, 0.3)) +
        scale_y_continuous(labels = percent_format()) +
        scale_x_continuous(breaks = seq(1, 200, 5)) +
        geom_ribbon(aes(ymin = conf_low, ymax = conf_high), fill = 'green', alpha=0.15) +
        geom_ribbon(aes(ymin = ifelse(conf_low <= 0, conf_low, NA), ymax = ifelse(conf_low <= 0, conf_high, NA)), fill = 'red', alpha=0.45) +
        geom_ribbon(aes(ymin = ifelse(conf_low > 0, conf_low, NA), ymax = ifelse(conf_low > 0, conf_high, NA)), fill = 'green', alpha=0.2) +
        geom_hline(yintercept = 0, color='red', alpha=0.5, size=1.5) +
        geom_vline(xintercept = actual_experiment_duration, color='orange', alpha=0.5, linetype="dashed", size=1.5) + 
        geom_vline(xintercept = maximum_experiment_duration, color='purple', alpha=0.5, linetype="dashed", size=1.5) + 
        geom_text(aes(label=percent(b_minus_a)), vjust=-1, check_overlap = TRUE) +    
        labs(title='Difference in Conversion Rate of `B` - `A`, with Frequentist Confidence Interval - \nNo Attribution Window',
             subtitle='Orange Dashed Line Represents End of Experiment at 60 days.\nPurple Dashed Line Represents End of Experiment at 90 Days.',
             y='Lift (i.e. Percent change from A to B)',
             x='Day of Experiment (and days after)') 
```

## Attribution Windows

So, we've seen the potential impact that diminishing lift can have.

But so far, we haven't considered how we can use attribution windows, or if we should.

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
attribution_window_days <- 5
```

Let's simulate a `r attribution_window_days`-day attribution window. In other word, we will only count conversion events that happen within `r attribution_window_days` days from the day that the person entered the experiment.

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
traffic <- traffic %>%
    mutate(converted_within_window = ifelse(conversion_offset <= attribution_window_days, converted, FALSE))

experiment_traffic <- traffic %>% filter(day_entered_experiment <= actual_experiment_duration)

conversion_table <- table(experiment_traffic$variation, experiment_traffic$converted_within_window)
conversions <- c(conversion_table['A', 'TRUE'], conversion_table['B', 'TRUE'])
non_conversions <- c(conversion_table['A', 'FALSE'], conversion_table['B', 'FALSE'])
trials <- conversions + non_conversions

a_conversion_rate <- conversions[1] / trials[1]
b_conversion_rate <- conversions[2] / trials[2]

b_percent_increase_over_baseline <- (b_conversion_rate - baseline_conversion_rate) / b_conversion_rate
b_percent_increase_over_a <- (b_conversion_rate - a_conversion_rate) / b_conversion_rate

experiment_traffic %>% 
    count(variation, converted_within_window) %>%
    group_by(variation) %>%
    mutate(conversion_rate = n / sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=variation, y=n, fill=converted_within_window)) +
    geom_col(position='dodge') +
    scale_y_continuous(labels = comma_format()) +
    scale_fill_manual(values = c("TRUE" = "#85CD75", "FALSE" = "#F57670")) +
    geom_text(aes(label=comma_format()(n)),
              position = position_dodge(width=0.9),
              vjust = 1.2) +
    geom_text(aes(label=percent_format()(conversion_rate)),
              position = position_dodge(width=0.9),
              vjust = -0.4) +
    labs(title=paste0('Simulated ', percent(baseline_conversion_rate),' Baseline Conversion Rate from Binomial Distribution,\nwith (diminishing) ', percent(simulated_percent_increase_in_b),' Lift from Effects of B variation &\n', attribution_window_days,'-Day Attribution Window.'),
         subtitle=paste0('This is the conversion rate after simulated the effect of diminishing Lift over time, with a', attribution_window_days,'-Day Attribution Window.'),
         y='Trials',
         x='Variation',
         fill='Converted') +
    theme_light()
```

The P-value is once again statistically significant: ``r prop.test(x=conversions, n=trials)$p.value``. (Although, to be fair, it might not have been if we didn't have a sufficient sample size.)

In this case, it appears we have reduced enough of the noise from the effects of the diminished lift and captured enough signal.

Now, `B`'s conversion rate is only ``r percent(b_conversion_rate)`` because we are only counting the people who converted within 7-days from when they entered they experiment. But, it is a ``r percent(b_percent_increase_over_a)`` increase over `A`'s conversion rate of ``r percent(a_conversion_rate)``.

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
p_value_traffic <- traffic %>%
    mutate(day_converted_within_window = ifelse(converted_within_window, day_entered_experiment + conversion_offset, NA))

num_days_people_are_still_converting <- max(p_value_traffic$day_entered_experiment) + max(p_value_traffic$conversion_offset) + attribution_window_days
experiment <- data.frame(experiment_day = NULL, variation=NULL, conversions = NULL, trials = NULL)

for(ex_day in 1:num_days_people_are_still_converting) {
    local_traffic <- p_value_traffic %>% filter(day_entered_experiment + attribution_window_days < ex_day)

    day_results <- rbind(data.frame(experiment_day = ex_day,
                                    variation="A",
                                    cumulative_conversions = sum(local_traffic$variation == "A" & local_traffic$day_converted_within_window <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(local_traffic$variation == "A")),
                         data.frame(experiment_day = ex_day,
                                    variation="B",
                                    cumulative_conversions = sum(local_traffic$variation == "B" & local_traffic$day_converted_within_window <= ex_day, na.rm = TRUE),
                                    cumulative_trials = sum(local_traffic$variation == "B")))

    experiment <- rbind(experiment, day_results)
}
```

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
experiment %>%
    mutate(cumulative_conversion_rate=cumulative_conversions / cumulative_trials) %>%
    ggplot(aes(x=experiment_day, y=cumulative_conversion_rate, color = variation, group=variation)) +
        geom_line() +
        geom_point() +
        geom_vline(xintercept = actual_experiment_duration, color='orange', alpha=0.5, linetype="dashed", size=1.5) + 
        geom_vline(xintercept = maximum_experiment_duration, color='purple', alpha=0.5, linetype="dashed", size=1.5) + 
        #geom_vline(xintercept = actual_experiment_duration, color='red', alpha=0.5) + 
        scale_y_continuous(breaks = seq(0, 1, 0.05),
                           labels = percent_format()) +
        expand_limits(y=c(0,0.12)) +
        scale_x_continuous(breaks = seq(0, 200, 5)) +
        scale_color_manual(values = c("A" = "#FF9500", "B" = "#19A6FF")) +
        labs(title='Conversion rate over time - With Attribution Window',
         subtitle='Orange Dashed Line Represents End of Experiment at 60 days.\nPurple Dashed Line Represents End of Experiment at 90 Days.',
         y='Conversion Rate',
         x='Day of Experiment (and days after)',
         color='Variation') +
    theme_light()
```

```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
cumulative_experiment <- experiment %>%
    #mutate(cumulative_conversion_rate=cumulative_conversions / cumulative_trials) %>%
  gather(variable, value, -(experiment_day:variation)) %>%
  unite(cumulative_experiment, variation, variable) %>%
  spread(cumulative_experiment, value)

prop.test_list <- with(cumulative_experiment, pmap(list(A_cumulative_conversions, B_cumulative_conversions, A_cumulative_trials, B_cumulative_trials),
                                    function(a, b, c, d) {
                                        if(a+b > 0) {
                                            prop.test(x=c(b, a), n=c(d, c))
                                        } else {
                                            NA    
                                        }
                                    }
                                    ))
cumulative_experiment$p_value <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$p.value   
    }
})
cumulative_experiment$b_minus_a <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         as.numeric(.$estimate[1] - .$estimate[2])
    }
})
cumulative_experiment$conf_low <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$conf.int[1]
    }
})
cumulative_experiment$conf_high <- map_dbl(prop.test_list,~  {
    
    if(is.na(.[[1]])) {
        return (NA)
    } else {
         .$conf.int[2]
    }
})

cumulative_experiment %>%
    ggplot(aes(x=experiment_day, y=p_value)) +
        geom_line() +
        geom_point() +
        geom_text(aes(label=round(p_value, 3)), vjust=-0.5, check_overlap = TRUE) +
        geom_hline(yintercept = 0.05, color ='red', alpha=0.5, size=1.5) +
        scale_x_continuous(breaks = seq(1, 200, 5)) +
        scale_y_continuous(breaks = seq(0, 1, 0.05)) +
        expand_limits(y=c(0,1)) +
        geom_vline(xintercept = actual_experiment_duration, color='orange', alpha=0.5, linetype="dashed", size=1.5) + 
        geom_vline(xintercept = maximum_experiment_duration, color='purple', alpha=0.5, linetype="dashed", size=1.5) + 
        labs(title='P-value over time - With Attribution Window',
            subtitle='Orange Dashed Line Represents End of Experiment at 60 days.\nPurple Dashed Line Represents End of Experiment at 90 Days.',
            y='Conversion Rate',
            x='Day of Experiment (and days after)',
            color='Variation') +
        theme_light()

cumulative_experiment %>%
    mutate(a_cr = A_cumulative_conversions / A_cumulative_trials,
           b_minus_a = b_minus_a / a_cr,
           conf_low = conf_low / a_cr,
           conf_high = conf_high / a_cr) %>%
    ggplot(aes(x=experiment_day, y=b_minus_a)) +
        geom_line() +
        coord_cartesian(ylim=c(-0.10, 0.3)) +
        scale_y_continuous(labels = percent_format()) +
        scale_x_continuous(breaks = seq(1, 200, 5)) +
        geom_ribbon(aes(ymin = conf_low, ymax = conf_high), fill = 'green', alpha=0.15) +
        geom_ribbon(aes(ymin = ifelse(conf_low <= 0, conf_low, NA), ymax = ifelse(conf_low <= 0, conf_high, NA)), fill = 'red', alpha=0.45) +
        geom_ribbon(aes(ymin = ifelse(conf_low > 0, conf_low, NA), ymax = ifelse(conf_low > 0, conf_high, NA)), fill = 'green', alpha=0.2) +
        geom_hline(yintercept = 0, color='red', alpha=0.5, size=1.5) +
        geom_vline(xintercept = actual_experiment_duration, color='orange', alpha=0.5, linetype="dashed", size=1.5) + 
        geom_vline(xintercept = maximum_experiment_duration, color='purple', alpha=0.5, linetype="dashed", size=1.5) + 
        geom_text(aes(label=percent(b_minus_a)), vjust=-1, check_overlap = TRUE) +
        labs(title='Difference in Conversion Rate of `B` - `A`, with Frequentist Confidence Interval - \nWith Attribution Window',
             subtitle='Orange Dashed Line Represents End of Experiment at 60 days.\nPurple Dashed Line Represents End of Experiment at 90 Days.',
             y='Lift (i.e. Percent change from A to B)',
             x='Day of Experiment (and days after)') 
```


## Conclusion

When running an A/B test, it is reasonable to assume that, if there is lift from the variant, the lift is highest in the first moments that it is seen by each person, and then decreases over time.

Under certain scenarios, this diminishing lift can influence the final results of the experiment and mask the immediate lift that the variant produces.

Also under certain scenarios, an attribution window can help extract the signal (of the immediate lift) and filter out the noise (of the diminished lift).

Please note: I'm **not** suggesting that the effect of diminishing lift is always present (although in most cases it seems like a safe assumption), or that it is always diminishing at the rate/distribution I assumed, or that the effects of it show up consistently in the way we see above. I'm simply showing what's possible, based on certain reasonable assumptions. I also don't explore possible ways to analyze A/B test data in general and determine the degree that diminishing lift affects the data or how to choose possible attribution windows. In addition, follow up research could include running the above scenarios many times (e.g. with different random seeds) to see how often the above issues occur, even with the same assumptions. Further work could also research the effects that attribution-windows have on conversion rates and, therefore, on the required sample-sizes (i.e. lower conversion rates recognized from the attribution window may lead to larger sample-sizes required, but larger lifts detected may lead to smaller sample-sizes required).
